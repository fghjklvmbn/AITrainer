model_type: "llama"
model_name_or_path: "beomi/KoAlpaca-llama-1-7b"
train_data_dir: "../data/json_files"
output_dir: "./checkpoints/llama"
batch_size: 2
epochs: 3
learning_rate: 5e-4
seed: 42
max_length: 512

lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - v_proj