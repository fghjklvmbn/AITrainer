model_name_or_path: "beomi/KoAlpaca-llama-1-7b"
train_batch_size: 2
eval_batch_size: 2
epochs: 3
learning_rate: 5e-4
seed: 42

lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - v_proj