training_args = TrainingArguments(
            output_dir=self.config["output_dir"],
            per_device_train_batch_size=self.config["batch_size"],
            num_train_epochs=self.config["epochs"],
            logging_dir=self.config.get("logging_dir", "./logs"),
            save_steps=100,
            save_total_limit=2,
            learning_rate=self.config["learning_rate"],
            fp16=self.config.get("fp16", False),  # 맥 혹은 cpu용으로 돌릴려면 False 혹은 주석 필요
            gradient_accumulation_steps=self.config.get("gradient_accumulation_steps", 1),
            warmup_ratio=self.config.get("warmup_ratio", 0.1),
            evaluation_strategy="steps",
            eval_steps=self.config.get("eval_steps", 500),
            save_strategy="steps",
            load_best_model_at_end=True,
            metric_for_best_model="eval_steps_per_second",
            greater_is_better=True,
            report_to="tensorboard",
            logging_steps=self.config.get("logging_steps", 100),
            dataloader_num_workers=self.config.get("dataloader_num_workers", 4),
            dataloader_pin_memory=self.config.get("dataloader_pin_memory", True),
            dataloader_drop_last=self.config.get("dataloader_drop_last", True),
            label_names=["labels"],
            remove_unused_columns=False,
            optim=self.config.get("optim", "adamw_torch"),
            lr_scheduler_type=self.config.get("lr_scheduler_type", "linear"),
            warmup_steps=self.config.get("warmup_steps", 0),
            max_steps=self.config.get("max_steps", -1),
            logging_first_step=self.config.get("logging_first_step", True),
            load_best_model_at_end=self.config.get("load_best_model_at_end", True),
            save_on_each_node=self.config.get("save_on_each_node", True),
            report_to=self.config.get("report_to", "tensorboard"),
            remove_unused_columns=self.config.get("remove_unused_columns", False),
            label_names=self.config.get("label_names", ["labels"]),
            optimizers=(self.config.get("optimizer"), self.config.get("lr_scheduler")),
            # Add any other training arguments you need
            # For example, you can add:
            # max_grad_norm=self.config.get("max_grad_norm", 1.0),
            # weight_decay=self.config.get("weight_decay", 0.01),
            # logging_dir=self.config.get("logging_dir", "./logs"),
            # logging_strategy=self.config.get("logging_strategy", "steps"),
            # logging_steps=self.config.get("logging_steps", 100),
            # save_strategy=self.config.get("save_strategy", "steps"),
            # save_steps=self.config.get("save_steps", 500),
            # save_total_limit=self.config.get("save_total_limit", 2),
            # evaluation_strategy=self.config.get("evaluation_strategy", "steps"),
            # eval_steps=self.config.get("eval_steps", 500),
            # load_best_model_at_end=self.config.get("load_best_model_at_end", True),
            # metric_for_best_model=self.config.get("metric_for_best_model", "eval_loss"),
            # greater_is_better=self.config.get("greater_is_better", False),
            # report_to=self.config.get("report_to", "tensorboard"), 
        )